{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Social Concerns Africa","text":"<p>Welcome to the Social Concerns Africa documentation site. This guide provides resources and step-by-step instructions for collecting, preparing, and analyzing social media data to extract valuable insights.</p>"},{"location":"#overview","title":"Overview","text":"<p>This documentation covers the following areas: - Data Collection: How to collect data from platforms like Twitter and YouTube. - Data Preparation: Methods to clean and preprocess collected data. - Extraction of Social Concerns: Techniques for topic modeling, document similarity, and sentiment analysis. - Analysis of Social Concerns: Temporal and social network analysis to uncover trends and patterns. - Visualization of Concerns: Building dashboards to present insights effectively.</p>"},{"location":"#quick-links","title":"Quick Links","text":"<ul> <li>Twitter Data Collection</li> <li>YouTube Data Collection</li> <li>Data Preparation</li> <li>Topic Modeling</li> <li>Sentiment Analysis</li> <li>Temporal Analysis</li> <li>Social Network Analysis</li> <li>Dashboard Visualization</li> </ul> <p>Explore the sidebar or use the links above to get started!</p>"},{"location":"1_Data_collection/twitter-data-collection/","title":"Twitter data collection","text":"<p>Apologies for the confusion earlier. Here's the corrected version of your <code>.md</code> file with the explanations and code blocks presented together without breaking them apart.</p>"},{"location":"1_Data_collection/twitter-data-collection/#twitter-data-collectionmd","title":"twitter-data-collection.md","text":"<pre><code># Twitter Data Collection Guide\n\nThis guide explains how to collect Twitter data without requiring a Twitter Developer Account using the `snscrape` tool.\n\n---\n\n## 1. Introduction\n\n`snscrape` is a command-line and Python library for scraping tweets. Unlike Twitter's API, it does not require developer credentials or an API key, making it accessible and easy to use for researchers.\n\n---\n\n## 2. Installation\n\nFollow these steps to install `snscrape`:\n\n### Requirements:\n- Python version: **3.8 or higher**\n\n### Install via pip:\n```bash\npip install git+https://github.com/JustAnotherArchivist/snscrape.git\n</code></pre>"},{"location":"1_Data_collection/twitter-data-collection/#3-usage-options","title":"3. Usage Options","text":"<p><code>snscrape</code> can be used in two ways:</p> <ol> <li>Command-Line Interface (CLI).</li> <li>Python Wrapper.</li> </ol>"},{"location":"1_Data_collection/twitter-data-collection/#31-using-the-cli","title":"3.1 Using the CLI","text":"<p>You can collect tweets directly using the command line. Run the following command:</p> <pre><code>snscrape --max-results 100 twitter-search \"covid-19 lang:en until:2022-12-31\" &gt; tweets.json\n</code></pre> <p>Explanation of Parameters: - <code>--max-results 100</code>: Limits the number of tweets collected to 100. - <code>twitter-search</code>: Specifies the search query for Twitter data. - <code>\"covid-19 lang:en until:2022-12-31\"</code>: Searches for English-language tweets containing \"covid-19\" and posted before December 31, 2022. - <code>&gt; tweets.json</code>: Saves the collected tweets to a file named <code>tweets.json</code>.</p>"},{"location":"1_Data_collection/twitter-data-collection/#32-using-the-python-wrapper","title":"3.2 Using the Python Wrapper","text":"<p>For more control and flexibility, you can use the Python API provided by <code>snscrape</code>. Below is an example script:</p> <pre><code>import snscrape.modules.twitter as sntwitter\n\n# Define the search query\nquery = \"covid-19 lang:en since:2020-12-01 until:2022-12-31\"\nmax_tweets = 100\n\n# Initialize an empty list to store tweets\ntweets = []\n\n# Use the TwitterSearchScraper to fetch tweets\nfor i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n    if i &gt;= max_tweets:  # Stop when max_tweets is reached\n        break\n    tweets.append([tweet.id, tweet.date, tweet.content])  # Save tweet data as a list\n\n# Display the first tweet for verification\nprint(tweets[0])\n</code></pre> <p>Explanation of the Code: 1. <code>query</code>: Defines the search criteria for tweets, including language (<code>lang:en</code>) and the date range (<code>since</code> and <code>until</code>). 2. <code>max_tweets</code>: Sets the maximum number of tweets to retrieve. 3. <code>TwitterSearchScraper</code>: A method from <code>snscrape</code> that fetches tweets based on the query. 4. <code>tweets.append</code>: Stores the tweet ID, date, and content in a list. 5. <code>print(tweets[0])</code>: Displays the first tweet in the collected data.</p>"},{"location":"1_Data_collection/twitter-data-collection/#4-saving-data-to-a-csv-file","title":"4. Saving Data to a CSV File","text":"<p>Once you've collected the tweets, it\u2019s useful to save them in a structured format like CSV for further analysis. Here\u2019s how to do it:</p> <pre><code>import pandas as pd\n\n# Convert the list of tweets to a DataFrame\ndf = pd.DataFrame(tweets, columns=[\"ID\", \"Date\", \"Content\"])\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"tweets.csv\", index=False)\n</code></pre> <p>Explanation of the Code: 1. <code>pd.DataFrame</code>: Converts the collected tweets into a tabular format with columns for tweet ID, date, and content. 2. <code>to_csv</code>: Saves the DataFrame to a CSV file named <code>tweets.csv</code>. 3. <code>index=False</code>: Prevents the addition of an unnecessary index column in the CSV file.</p>"},{"location":"1_Data_collection/twitter-data-collection/#5-next-steps","title":"5. Next Steps","text":"<p>After collecting and saving the tweets, you can perform various analyses, such as:</p> <ul> <li>Sentiment Analysis: Use Natural Language Processing (NLP) techniques to analyze the sentiment of tweets.</li> <li>Topic Modeling: Apply algorithms like Latent Dirichlet Allocation (LDA) to identify common themes.</li> <li>Network Analysis: Explore relationships between users or hashtags.</li> </ul> <p>These analyses can help you derive insights from the data you collected.</p>"},{"location":"1_Data_collection/twitter-data-collection/#6-summary","title":"6. Summary","text":"<p>In this guide, we covered:</p> <ol> <li>Installation and setup of <code>snscrape</code>.</li> <li>Using both the CLI and Python wrapper to scrape tweets.</li> <li>Saving the collected data to a CSV file for further analysis.</li> </ol> <p>With this knowledge, you are now ready to collect and analyze Twitter data! ```</p>"},{"location":"1_Data_collection/youtube_data_collection/","title":"YouTube Data Collection","text":"<p>In this section, we will describe how we collected YouTube data related to social concerns in Africa. We used the YouTube Data API and web scraping techniques to gather the necessary data for analysis.</p>"},{"location":"1_Data_collection/youtube_data_collection/#1-introduction","title":"1. Introduction","text":"<p>We collected YouTube data using two methods:  - YouTube Data API: To fetch video statistics, tags, and category details. - Web Scraping (Selenium): To extract additional information such as video descriptions, titles, and links.</p>"},{"location":"1_Data_collection/youtube_data_collection/#2-code-for-collecting-youtube-data","title":"2. Code for Collecting YouTube Data","text":"<p>We used the <code>youtube_search</code> function defined in the <code>youtube_data.py</code> file to search for videos related to social concerns. The function collects data such as video title, view count, comment count, etc. Here's the code used for this process:</p>"},{"location":"1_Data_collection/youtube_data_collection/#importing-necessary-libraries","title":"Importing Necessary Libraries","text":"<pre><code>import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom youtube_data import youtube_search\n</code></pre>"},{"location":"1_Data_collection/youtube_data_collection/#defining-the-read_youtube-function","title":"Defining the <code>read_youtube</code> Function","text":"<p>This function takes a list of keywords, a start year, and an end year to search YouTube for videos related to social concerns.</p> <pre><code>def read_youtube(list_title, start_year, end_year):\n    tests = []\n    for i in range(start_year, end_year + 1):\n        for title in list_title:\n            tests.append(youtube_search(\"social \" + title + \", Africa, \" + str(i)))\n    return tests\n\ntest = read_youtube([\"concern\", \"problem\", \"challenge\", \"worry\", \"issue\", \"question\"], 2020, 2020)\n\ndf_test = []\nfor t in test:\n    t.keys()\n    df_test.append(pd.DataFrame(data=t))\n\ndf = pd.concat(df_test)\ndf.to_excel(\"concerns120202020.xlsx\")\n</code></pre>"},{"location":"1_Data_collection/youtube_data_collection/#processing-and-saving-data","title":"Processing and Saving Data","text":"<p>We processed the data collected and saved it to both Excel and CSV formats for further analysis.</p> <pre><code>df_test = []\nfor t in test:\n    t.keys()\n    df_test.append(pd.DataFrame(data=t))\n\ndf = pd.concat(df_test)\ndf.to_excel(\"concerns120202020.xlsx\")\ndf.to_csv('concerns20202020.csv', sep=',', index=False)\n</code></pre>"},{"location":"1_Data_collection/youtube_data_collection/#code-for-scraping-youtube-data-using-selenium","title":"Code for Scraping YouTube Data Using Selenium","text":"<p>We used Selenium WebDriver to scrape video details such as title and description for each video. The following code demonstrates how we used Selenium to extract this information:</p> <pre><code>from selenium import webdriver \nimport pandas as pd \nfrom selenium.webdriver.common.by import By \nfrom selenium.webdriver.chrome.options import Options\n\ndriver_path = \"chromedriver101.exe\"\noption = Options()\noption.add_argument(\"--disable-infobars\")\noption.add_argument(\"start-maximized\")\noption.add_argument(\"--disable-extensions\")\n\ndriver = webdriver.Chrome(executable_path=driver_path, options=option)\n\nfor i in range(1, 2): # Iterate through pages\n    driver.get(\"https://www.youtube.com/results?search_query=social+concerns%2C+Africa%2C+2018\")\n\n    user_data = driver.find_elements(By.XPATH, '//*[@id=\"video-title\"]')\n    links = []\n    for i in user_data:\n        links.append(i.get_attribute('href'))\n\n    print(len(links))\n\n    df = pd.DataFrame(columns=['link', 'title', 'description', 'category'])\n\n    for x in links:\n        driver.get(x)\n        v_id = x.strip('https://www.youtube.com/watch?v=')\n        v_title = driver.find_element(By.CSS_SELECTOR, \"h3#video-title\").text\n        v_description = driver.find_element(By.CSS_SELECTOR, \"span#description-text\").text\n        df.loc[len(df)] = [v_id, v_title, v_description, 'concerns']\n\ndf.to_excel(\"youconcerns120182.xlsx\")\n</code></pre>"},{"location":"1_Data_collection/youtube_data_collection/#saving-the-scraped-data","title":"Saving the Scraped Data","text":"<p>Finally, we save the scraped data into a CSV file:</p> <pre><code>df.to_csv(\"youtube_concerns.csv\", encoding='utf-8', index=False)\n</code></pre>"}]}