{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"twitter-data-collection/","title":"Home","text":"<p>Apologies for the confusion earlier. Here's the corrected version of your <code>.md</code> file with the explanations and code blocks presented together without breaking them apart.</p>"},{"location":"twitter-data-collection/#twitter-data-collectionmd","title":"twitter-data-collection.md","text":"<pre><code># Twitter Data Collection Guide\n\nThis guide explains how to collect Twitter data without requiring a Twitter Developer Account using the `snscrape` tool.\n\n---\n\n## 1. Introduction\n\n`snscrape` is a command-line and Python library for scraping tweets. Unlike Twitter's API, it does not require developer credentials or an API key, making it accessible and easy to use for researchers.\n\n---\n\n## 2. Installation\n\nFollow these steps to install `snscrape`:\n\n### Requirements:\n- Python version: **3.8 or higher**\n\n### Install via pip:\n```bash\npip install git+https://github.com/JustAnotherArchivist/snscrape.git\n</code></pre>"},{"location":"twitter-data-collection/#3-usage-options","title":"3. Usage Options","text":"<p><code>snscrape</code> can be used in two ways:</p> <ol> <li>Command-Line Interface (CLI).</li> <li>Python Wrapper.</li> </ol>"},{"location":"twitter-data-collection/#31-using-the-cli","title":"3.1 Using the CLI","text":"<p>You can collect tweets directly using the command line. Run the following command:</p> <pre><code>snscrape --max-results 100 twitter-search \"covid-19 lang:en until:2022-12-31\" &gt; tweets.json\n</code></pre> <p>Explanation of Parameters: - <code>--max-results 100</code>: Limits the number of tweets collected to 100. - <code>twitter-search</code>: Specifies the search query for Twitter data. - <code>\"covid-19 lang:en until:2022-12-31\"</code>: Searches for English-language tweets containing \"covid-19\" and posted before December 31, 2022. - <code>&gt; tweets.json</code>: Saves the collected tweets to a file named <code>tweets.json</code>.</p>"},{"location":"twitter-data-collection/#32-using-the-python-wrapper","title":"3.2 Using the Python Wrapper","text":"<p>For more control and flexibility, you can use the Python API provided by <code>snscrape</code>. Below is an example script:</p> <pre><code>import snscrape.modules.twitter as sntwitter\n\n# Define the search query\nquery = \"covid-19 lang:en since:2020-12-01 until:2022-12-31\"\nmax_tweets = 100\n\n# Initialize an empty list to store tweets\ntweets = []\n\n# Use the TwitterSearchScraper to fetch tweets\nfor i, tweet in enumerate(sntwitter.TwitterSearchScraper(query).get_items()):\n    if i &gt;= max_tweets:  # Stop when max_tweets is reached\n        break\n    tweets.append([tweet.id, tweet.date, tweet.content])  # Save tweet data as a list\n\n# Display the first tweet for verification\nprint(tweets[0])\n</code></pre> <p>Explanation of the Code: 1. <code>query</code>: Defines the search criteria for tweets, including language (<code>lang:en</code>) and the date range (<code>since</code> and <code>until</code>). 2. <code>max_tweets</code>: Sets the maximum number of tweets to retrieve. 3. <code>TwitterSearchScraper</code>: A method from <code>snscrape</code> that fetches tweets based on the query. 4. <code>tweets.append</code>: Stores the tweet ID, date, and content in a list. 5. <code>print(tweets[0])</code>: Displays the first tweet in the collected data.</p>"},{"location":"twitter-data-collection/#4-saving-data-to-a-csv-file","title":"4. Saving Data to a CSV File","text":"<p>Once you've collected the tweets, it\u2019s useful to save them in a structured format like CSV for further analysis. Here\u2019s how to do it:</p> <pre><code>import pandas as pd\n\n# Convert the list of tweets to a DataFrame\ndf = pd.DataFrame(tweets, columns=[\"ID\", \"Date\", \"Content\"])\n\n# Save the DataFrame to a CSV file\ndf.to_csv(\"tweets.csv\", index=False)\n</code></pre> <p>Explanation of the Code: 1. <code>pd.DataFrame</code>: Converts the collected tweets into a tabular format with columns for tweet ID, date, and content. 2. <code>to_csv</code>: Saves the DataFrame to a CSV file named <code>tweets.csv</code>. 3. <code>index=False</code>: Prevents the addition of an unnecessary index column in the CSV file.</p>"},{"location":"twitter-data-collection/#5-next-steps","title":"5. Next Steps","text":"<p>After collecting and saving the tweets, you can perform various analyses, such as:</p> <ul> <li>Sentiment Analysis: Use Natural Language Processing (NLP) techniques to analyze the sentiment of tweets.</li> <li>Topic Modeling: Apply algorithms like Latent Dirichlet Allocation (LDA) to identify common themes.</li> <li>Network Analysis: Explore relationships between users or hashtags.</li> </ul> <p>These analyses can help you derive insights from the data you collected.</p>"},{"location":"twitter-data-collection/#6-summary","title":"6. Summary","text":"<p>In this guide, we covered:</p> <ol> <li>Installation and setup of <code>snscrape</code>.</li> <li>Using both the CLI and Python wrapper to scrape tweets.</li> <li>Saving the collected data to a CSV file for further analysis.</li> </ol> <p>With this knowledge, you are now ready to collect and analyze Twitter data! ```</p>"}]}